{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from utils import *\n",
    "import os, sys, time, datetime, random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path='config/yolov3.cfg'\n",
    "weights_path='weights/yolov3.weights'\n",
    "class_path='data/coco.names'\n",
    "\n",
    "img_dir = '/n/fs/visualai-scr/Data/UCF101Images/'\n",
    "UCF_classes = os.listdir(img_dir)\n",
    "UCF_classes.remove('ucfTrainTestlist')\n",
    "img_size=320\n",
    "conf_thres=0.3\n",
    "nms_thres=0.4\n",
    "\n",
    "# Load model and weights\n",
    "model = Darknet(config_path, img_size=img_size)\n",
    "model.load_darknet_weights(weights_path)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "classes = utils.load_classes(class_path)\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "print('Done loading in model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_list(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i+n]\n",
    "\n",
    "def load_image(filepath):\n",
    "    return Image.open(filepath)\n",
    "\n",
    "def detect_image(img):\n",
    "    imw = 320\n",
    "    imh = 240\n",
    "    img_transforms=transforms.Compose([\n",
    "         transforms.Pad((max(int((imh-imw)/2),0), \n",
    "              max(int((imw-imh)/2),0), max(int((imh-imw)/2),0),\n",
    "              max(int((imw-imh)/2),0)), (128,128,128)),\n",
    "         transforms.ToTensor(),\n",
    "         ])\n",
    "    # convert image to Tensor\n",
    "    image_tensor = img_transforms(img).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    input_img = Variable(image_tensor.type(Tensor))\n",
    "    # run inference on the model and get detections\n",
    "    with torch.no_grad():\n",
    "        detections = model(input_img)\n",
    "        detections = utils.non_max_suppression(detections, \n",
    "                        conf_thres, nms_thres)\n",
    "    return detections[0]\n",
    "\n",
    "def detect_images(image_list):\n",
    "    imw = 320\n",
    "    imh = 240\n",
    "    img_transforms=transforms.Compose([\n",
    "    transforms.Pad((max(int((imh-imw)/2),0), \n",
    "         max(int((imw-imh)/2),0), max(int((imh-imw)/2),0),\n",
    "         max(int((imw-imh)/2),0)), (128,128,128)),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "    image_tensors = []\n",
    "    for img in image_list:\n",
    "        image_tensor = img_transforms(img).float()\n",
    "        image_tensor = image_tensor.unsqueeze_(0)\n",
    "        image_tensors.append(image_tensor)\n",
    "    image_tensors = torch.cat(image_tensors, 0)\n",
    "    input_img = Variable(image_tensors.type(Tensor))\n",
    "    with torch.no_grad():\n",
    "        detections = model(input_img)\n",
    "        detections = utils.non_max_suppression(detections, \n",
    "                        conf_thres, nms_thres)\n",
    "    return detections\n",
    "\n",
    "def grey_out_detection(img, detections):\n",
    "    img_np = np.array(img)\n",
    "    img_mean = np.mean(img, axis=(0,1))\n",
    "    mask = np.zeros(img_np.shape[0:2])\n",
    "    detect_person = 0\n",
    "    if detections is not None:\n",
    "        for detection in detections:\n",
    "            if detection[-1] == 0.0:\n",
    "                detect_person = 1\n",
    "                bbox = detection[:4].cpu().numpy().astype(np.uint16)\n",
    "                bbox[1], bbox[3] = bbox[1] - 40, bbox[3] - 40\n",
    "                mask[bbox[1]:bbox[3],bbox[0]:bbox[2]] = 1\n",
    "    \n",
    "    img_np[mask == 0,:] = img_mean\n",
    "    return Image.fromarray(img_np), detect_person\n",
    "\n",
    "def grey_out_detections(image_list, detections):\n",
    "    greyed_image_list = []\n",
    "    n_frame_w_person = 0\n",
    "    for i, image in enumerate(image_list):\n",
    "        detection = detections[i]\n",
    "        greyed_image, detect_person = grey_out_detection(image, detection)\n",
    "        greyed_image_list.append(greyed_image)\n",
    "        n_frame_w_person += detect_person\n",
    "    return greyed_image_list, n_frame_w_person\n",
    "\n",
    "def grey_out_video(frame_dir):\n",
    "    out_frame_dir = frame_dir.replace('UCF101Images', 'UCF101ImagesGreyed')\n",
    "    os.makedirs(out_frame_dir, exist_ok=True)\n",
    "    frame_names = os.listdir(frame_dir)\n",
    "    if len(os.listdir(out_frame_dir)) == len(frame_names):\n",
    "        print('Video {} has already been processed. Skipping.'.format(frame_dir.split('/')[-1]))\n",
    "        return\n",
    "    \n",
    "    prev_time = time.time()\n",
    "    frame_names_chunked = list(chunk_list(frame_names, 16))\n",
    "    total_frames_w_person = 0\n",
    "    for chunk in frame_names_chunked:\n",
    "        image_list = []\n",
    "        for frame_name in chunk:\n",
    "            frame_path = os.path.join(frame_dir, frame_name)\n",
    "            image_list.append(load_image(frame_path))\n",
    "        detections = detect_images(image_list)\n",
    "        greyed_image_list, n_frame_w_person = grey_out_detections(image_list, detections)\n",
    "        total_frames_w_person += n_frame_w_person\n",
    "        for i, frame_name in enumerate(chunk):\n",
    "            out_frame_path = os.path.join(out_frame_dir, frame_name)\n",
    "            greyed_image_list[i].save(out_frame_path)\n",
    "    inference_time = datetime.timedelta(seconds=time.time() - prev_time)\n",
    "    print ('Done processing Video {}. Time Taken: {}'.format(frame_dir.split('/')[-1], inference_time))\n",
    "    return float(total_frames_w_person)/len(frame_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating list of videos to process.\n",
      "Done. Processing 13320 videos\n"
     ]
    }
   ],
   "source": [
    "print('Creating list of videos to process.')\n",
    "video_list = []\n",
    "for class_name in UCF_classes:\n",
    "    video_dir   = os.path.join(img_dir, class_name)\n",
    "    video_names = os.listdir(video_dir)\n",
    "    for video_name in video_names:\n",
    "        frame_dir   = os.path.join(video_dir, video_name)\n",
    "        video_list.append(frame_dir)\n",
    "print('Done. Processing {} videos'.format(len(video_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing Video v_JugglingBalls_g02_c02. Time Taken: 0:00:05.687844\n",
      "Done processing Video v_JugglingBalls_g11_c03. Time Taken: 0:00:05.528441\n",
      "Done processing Video v_JugglingBalls_g21_c04. Time Taken: 0:00:05.714656\n"
     ]
    }
   ],
   "source": [
    "person_fraction_fname = 'person_fraction.txt'\n",
    "for frame_dir in video_list[0:3]:\n",
    "    person_fraction_file = open(person_fraction_fname, 'a+')\n",
    "    total_frames_w_person = grey_out_video(frame_dir)\n",
    "    person_fraction_file.write('{}\\t{}\\n'.format(frame_dir.split('/')[-1], total_frames_w_person))\n",
    "    person_fraction_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
